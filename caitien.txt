Build Mr.HelpMate AI
If you prefer a structured project with a clear starting point, you can build a project in the insurance domain, similar the project you saw in the "retrieval augmented generation" session. The goal of the project will be to build a robust generative search system capable of effectively and accurately answering questions from a policy document. 

pdf_path=./Principal-Sample-Life-Insurance-Policy.pdf

The project should implement all the three layers effectively. It will be key to try out various strategies and experiments in various layers in order to build an effective search system. Let's explore what you need to do in each of the layers, and the possible experimentations that you can perform based on various choices.

The Embedding Layer: The PDF document needs to be effectively processed, cleaned, and chunked for the embeddings. Here, the choice of the chunking strategy will have a large impact on the final quality of the retrieved results. So, make sure that you try out various stratgies and compare their performances.

Another important aspect in the embedding layer is the choice of the embedding model. You can choose to embed your chunks using the OpenAI embedding model or any model from the SentenceTransformers library on HuggingFace.
 
The Search Layer: Here, you first need to design at least 3 queries against which you will test your system. You need to understand and skim through the document, and accordingly come up with some queries, the answers to which can be found in the policy document.

Next, you need to embed the queries and search your ChromaDB vector database against each of these queries. Implementing a cache mechanism is also mandatory.

Finally, you need to implement the re-ranking block, and for this you can choose from a range of cross-encoding models on HuggingFace.
 
The Generation Layer: In the generation layer, the final prompt that you design is the major component. Make sure that the prompt is exhaustive in its instructions, and the relevant information is correctly passed to the prompt. You may also choose to provide some few-shot examples in an attempt to improve the LLM output.



--------------------------------------------------------
    Lưu trữ dữ liệu trích xuất từ PDF vào ChromaDB ngay sau khi trích xuất.
    Khi một câu hỏi được hỏi, hệ thống sẽ tìm kiếm trong cache trước. Nếu không tìm thấy trong cache, hệ thống sẽ query trong ChromaDB. Nếu kết quả không tìm thấy trong ChromaDB, hệ thống sẽ trích xuất lại từ file PDF và lưu vào ChromaDB.




1. Thêm đoạn mã kiểm tra xem trong chromadb xem đã có dữ liệu lưu chưa. nếu có trích xuất pdf và lưu vào chromadb. (cần chắc chắn db phải có dữ liệu trước khi nhận câu hỏi.). nếu trong chromadb đã có thì tiếp các bước thứ 2 và 3
2. ưu trữ dữ liệu trích xuất từ PDF vào ChromaDB ngay sau khi trích xuất.
3. Khi một câu hỏi được hỏi, hệ thống sẽ tìm kiếm trong cache trước. Nếu không tìm thấy trong cache, hệ thống sẽ query trong ChromaDB. Nếu kết quả không tìm thấy trong ChromaDB, hệ thống sẽ trích xuất lại từ file PDF và lưu vào ChromaDB.


-----------------------------------------------------------------------

1. Function to check whether a word is present in a table or not for segregation of regular text and tables
    same code:

# Function to check whether a word is present in a table or not for segregation of regular text and tables

def check_bboxes(word, table_bbox):
    # Check whether word is inside a table bbox.
    l = word['x0'], word['top'], word['x1'], word['bottom']
    r = table_bbox
    return l[0] > r[0] and l[1] > r[1] and l[2] < r[2] and l[3] < r[3]


2. # Function to extract text from a PDF file.
# 1. Declare a variable p to store the iteration of the loop that will help us store page numbers alongside the text
# 2. Declare an empty list 'full_text' to store all the text files
# 3. Use pdfplumber to open the pdf pages one by one
# 4. Find the tables and their locations in the page
# 5. Extract the text from the tables in the variable 'tables'
# 6. Extract the regular words by calling the function check_bboxes() and checking whether words are present in the table or not
# 7. Use the cluster_objects utility to cluster non-table and table words together so that they retain the same chronology as in the original PDF
# 8. Declare an empty list 'lines' to store the page text
# 9. If a text element in present in the cluster, append it to 'lines', else if a table element is present, append the table
# 10. Append the page number and all lines to full_text, and increment 'p'
# 11. When the function has iterated over all pages, return the 'full_text' list

    same code: 
                def extract_text_from_pdf(pdf_path):
    p = 0
    full_text = []


    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            page_no = f"Page {p+1}"
            text = page.extract_text()

            tables = page.find_tables()
            table_bboxes = [i.bbox for i in tables]
            tables = [{'table': i.extract(), 'top': i.bbox[1]} for i in tables]
            non_table_words = [word for word in page.extract_words() if not any(
                [check_bboxes(word, table_bbox) for table_bbox in table_bboxes])]
            lines = []

            for cluster in pdfplumber.utils.cluster_objects(non_table_words + tables, itemgetter('top'), tolerance=5):

                if 'text' in cluster[0]:
                    try:
                        lines.append(' '.join([i['text'] for i in cluster]))
                    except KeyError:
                        pass

                elif 'table' in cluster[0]:
                    lines.append(json.dumps(cluster[0]['table']))


            full_text.append([page_no, " ".join(lines)])
            p +=1

    return full_text

3. # Retain only the rows with a text length of at least 10
        samecode:
                    insurance_pdfs_data = insurance_pdfs_data.loc[insurance_pdfs_data['Text_Length'] >= 10]
                    insurance_pdfs_data.head()

This concludes the chunking aspect also, as we can see that mostly the pages contain few hundred words, maximum going upto 1000. So, we don't need to chunk the documents further; we can perform the embeddings on individual pages. This strategy makes sense for 2 reasons:
1. The way insurance documents are generally structured, you will not have a lot of extraneous information in a page, and all the text pieces in that page will likely be interrelated.
2. We want to have larger chunk sizes to be able to pass appropriate context to the LLM during the generation layer.

4. Generate and Store Embeddings using OpenAI and ChromaDB

In this section, we will embed the pages in the dataframe through OpenAI's `text-embedding-ada-002` model, and store them in a ChromaDB collection.

Same code: # Import the OpenAI Embedding Function into chroma

            from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction

            # Call PersistentClient()

            client = chromadb.PersistentClient(path=chroma_data_path)
            # Set up the embedding function using the OpenAI embedding model

            model = "text-embedding-ada-002"
            embedding_function = OpenAIEmbeddingFunction(api_key=openai.api_key, model_name=model)

            from chromadb.utils import embedding_functions
            sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name="all-MiniLM-L6-v2")

            #  Convert the page text and metadata from your dataframe to lists to be able to pass it to chroma

            documents_list = insurance_pdfs_data["Page_Text"].tolist()
            metadata_list = insurance_pdfs_data['Metadata'].tolist()

# Add the documents and metadata to the collection alongwith generic integer IDs. You can also feed the metadata information as IDs by combining the policy name and page no.

insurance_collection.add(
    documents= documents_list,
    ids = [str(i) for i in range(0, len(documents_list))],
    metadatas = metadata_list
)

# Let's take a look at the first few entries in the collection

insurance_collection.get(
    ids = ['0','1','2'],
    include = ['embeddings', 'documents', 'metadatas']
)

cache_collection = client.get_or_create_collection(name='Insurance_Cache', embedding_function=embedding_function)

cache_collection.peek()



once the text in the documents has been pre-processed and chunked, the next step is to generate vector representations using a suitable text embedding model. So far, you have been using the sentence transformer library and, specifically, the all-MiniLM-L6-V2 model to generate vector embeddings. For this demonstration, the embedding model being used is OpenAI's embedding model - specifically, the ada002 v2 model, which embeds text into a vector of 1,536 dimensions. We are using ChromaDB’s utilities functions to generate the vector embeddings through OpenAI’s model. For more information on this, refer to :

https://docs.trychroma.com/embeddings#openai


Once the embeddings have been generated, the next step is to store them in the vector database, which is ChromaDB. As covered in the previous sessions on ChromaDB, you need to first create the Chroma collections before you can start adding documents. Akshay used the get_or_create_collections method, which will create a collection if not already present, and fetch it from your system if it has been created and stored previously. Next, since we are using OpenAI embeddings and not Chroma's default embedding, you need to also pass your embedding function as an argument while creating the collection. Finally, the information that includes the document list, text and metadata information is passed to the chroma collection. Additionally, Akshay also created a Chroma collection to serve as cache, which we will explore in the upcoming segment.

https://medium.com/@azhar.sayyad6/a-step-by-step-guide-to-parsing-pdfs-using-the-pdfplumber-library-in-python-c12d94ae9f07#:~:text=pdfplumber%20is%20a%20powerful%20library,data%20analysis%20and%20automation%20tasks.


Semantic Search Layer

As with any good system design, we need to consider a scenario when the application is scaled - suppose the number of documents increases or multiple users are using the application. Such a scenario opens up multiple concerns about the system’s performance

- How will the system handle multiple queries simultaneously?
- Is there scope to improve the system’s overall performance in search and retrieval?



The first concern can be solved by using vector databases and scaling up the compute units (clusters/server) for the application. For the second concern, an improvement to the overall system design is required which can be achieved by implementing a cache collection in the vector database that stores previous queries and their results in the vector database. Let’s hear more on this from Akshay in the video below.




--------------------------------------------------------------

Tôi sẽ bắt đầu bằng việc đọc và phân tích nội dung của file main.py bạn đã tải lên. Sau đó, tôi sẽ đề xuất cách tiếp cận để cải tiến việc gửi file PDF đến GPT cho việc trả lời các câu hỏi. Đầu tiên, hãy xem nội dung của file.

File main.py mà bạn đã tải lên chứa một ứng dụng FastAPI để trích xuất văn bản từ file PDF, làm sạch và phân đoạn văn bản, sau đó sử dụng các embeddings văn bản và FAISS index để tìm kiếm và trả lời các câu hỏi. Nó cũng tích hợp khả năng gửi câu hỏi đến OpenAI để nhận câu trả lời.

Cải tiến quy trình này để gửi file PDF đến GPT cho việc trả lời câu hỏi có thể bao gồm các bước sau:

    Tối ưu hóa việc trích xuất văn bản từ PDF:
        Cân nhắc sử dụng thêm các thư viện hoặc cải thiện thuật toán hiện tại để tăng tốc độ và chất lượng trích xuất văn bản, nhất là với các file PDF có nhiều hình ảnh và định dạng phức tạp.

    Cải thiện việc làm sạch và phân đoạn văn bản:
        Áp dụng các kỹ thuật NLP tiên tiến hơn để làm sạch và phân đoạn văn bản, giúp giữ lại thông tin quan trọng và loại bỏ nhiễu.

    Nâng cao chất lượng embeddings và FAISS index:
        Cập nhật model embeddings văn bản sang phiên bản mới hơn hoặc tinh chỉnh model hiện tại để cải thiện chất lượng embeddings.
        Tối ưu hóa FAISS index để tăng tốc độ tìm kiếm và độ chính xác.

    Tích hợp trực tiếp với OpenAI API:
        Cập nhật đoạn code giao tiếp với OpenAI API để sử dụng các model GPT mới nhất và tận dụng các cài đặt tiên tiến như temperature, max tokens, etc., để cải thiện chất lượng câu trả lời.

    Cache và tối ưu hóa truy vấn:
        Thêm cơ chế cache cho các bước nặng như trích xuất và xử lý văn bản, embeddings và tìm kiếm FAISS để giảm thời gian xử lý cho các truy vấn lặp lại.

    Giao diện người dùng và trải nghiệm:
        Cải thiện giao diện người dùng cho việc tải lên PDF và nhận câu trả lời, bao gồm cả việc hiển thị tiến trình và kết quả một cách trực quan.

    Xử lý lỗi và Logging:
        Thêm xử lý lỗi chi tiết và logging để dễ dàng gỡ lỗi và tối ưu hóa hiệu suất ứng dụng.

Cần chú ý đến việc giữ thông tin cá nhân và dữ liệu nhạy cảm an toàn, tuân thủ các quy định về bảo mật và quyền riêng tư.

Bạn có muốn tôi giúp bạn thực hiện cụ thể một trong những cải tiến này không? ​
​

-------------------------------------------------------------1---------------------------------------------------------------
Để tối ưu hóa việc trích xuất văn bản từ PDF trong file main.py bạn đã gửi, ta có thể thực hiện các cải tiến sau:

    Sử dụng Thư Viện Trích Xuất Văn Bản Mới Hoặc Cải Thiện:
        Kiểm tra phiên bản mới nhất của PyMuPDF (fitz) để đảm bảo bạn đang sử dụng các tính năng và cải tiến mới nhất. Cập nhật phiên bản nếu cần.
        Cân nhắc áp dụng các cài đặt nâng cao trong fitz để tối ưu hóa việc trích xuất, ví dụ như tinh chỉnh các tham số trích xuất dựa trên loại nội dung của PDF.

    Tối ưu Hóa Đoạn Code Trích Xuất:
        Trích xuất văn bản từ mỗi trang PDF có thể được song song hóa để tăng tốc độ xử lý, đặc biệt là với các tài liệu PDF dài.

    Xử Lý Văn Bản Đồ Họa và Định Dạng Phức Tạp:
        Nếu PDF chứa nhiều hình ảnh, biểu đồ hoặc bảng, bạn có thể cân nhắc sử dụng thêm các công cụ như OCR (Optical Character Recognition) để trích xuất văn bản từ hình ảnh.




